{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonosc import udp_client\n",
    "import time\n",
    "import sounddevice as sd\n",
    "import torch\n",
    "from dataloaders.beat import CustomDataset\n",
    "from dataloaders.build_vocab import Vocab\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from utils.other_tools import load_checkpoints\n",
    "from models.camn import CaMN\n",
    "\n",
    "camn_config_file = open(\"camn_config.obj\", 'rb') \n",
    "gesturegen_config_file = open(\"gesturegen_config.obj\", 'rb')\n",
    "\n",
    "gesturegen_args = pickle.load(gesturegen_config_file)\n",
    "camn_args = pickle.load(camn_config_file)\n",
    "\n",
    "mean_facial = torch.from_numpy(np.load(camn_args.root_path+camn_args.mean_pose_path+f\"{camn_args.facial_rep}/json_mean.npy\")).float()\n",
    "std_facial = torch.from_numpy(np.load(camn_args.root_path+camn_args.mean_pose_path+f\"{camn_args.facial_rep}/json_std.npy\")).float()\n",
    "mean_audio = torch.from_numpy(np.load(camn_args.root_path+camn_args.mean_pose_path+f\"{camn_args.audio_rep}/npy_mean.npy\")).float()\n",
    "std_audio = torch.from_numpy(np.load(camn_args.root_path+camn_args.mean_pose_path+f\"{camn_args.audio_rep}/npy_std.npy\")).float()\n",
    "mean_pose = torch.from_numpy(np.load(camn_args.root_path+camn_args.mean_pose_path+f\"{camn_args.pose_rep}/bvh_mean.npy\")).float()\n",
    "std_pose = torch.from_numpy(np.load(camn_args.root_path+camn_args.mean_pose_path+f\"{camn_args.pose_rep}/bvh_std.npy\")).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18_daiki_0_103_a.bvh\n"
     ]
    }
   ],
   "source": [
    "test_data = CustomDataset(camn_args, \"test\")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=1,  \n",
    "    shuffle=False,  \n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "batch_size = 1\n",
    "solo_speaker = 17\n",
    "\n",
    "for its, template in enumerate(test_loader):\n",
    "    if template['id'][0] == solo_speaker:\n",
    "        break\n",
    "\n",
    "test_demo = camn_args.root_path + camn_args.test_data_path + f\"{camn_args.pose_rep}_vis/\"\n",
    "test_seq_list = os.listdir(test_demo)\n",
    "test_seq_list.sort()\n",
    "\n",
    "template_bvh = test_seq_list[its]\n",
    "print(template_bvh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # load in facial model\n",
    "from scripts.MulticontextNet import GestureGen\n",
    "model_path = 'tmp/multicontextnet-no-text.pth'\n",
    "net = GestureGen(gesturegen_args)\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "net = net.cuda().eval()\n",
    "\n",
    "facial_norm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sample rate: 22050\n"
     ]
    }
   ],
   "source": [
    "# load in test_audio\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "test_audio_file = 'test_audio/gandhi-speech.wav'\n",
    "test_audio_raw, sr = librosa.load(test_audio_file, duration=30, sr=None) # np array\n",
    "sf.write('result_pose/res_gandhi_speech.wav', test_audio_raw, sr)\n",
    "\n",
    "test_audio = librosa.resample(test_audio_raw, orig_sr=sr, target_sr=16000) #test_audio_raw[::sr//16000] # convert to 16khz\n",
    "print('Original sample rate:', sr)\n",
    "out_audio = torch.from_numpy(test_audio).unsqueeze(0)\n",
    "audio = (out_audio - mean_audio) / std_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio finished: 1718236856.2999249\n"
     ]
    }
   ],
   "source": [
    "limit_sec = 10\n",
    "sd.play(test_audio[0:limit_sec*16000], 16000)\n",
    "sd.wait()\n",
    "print(\"Audio finished:\", time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_audio = audio.expand(batch_size, -1).cuda()\n",
    "in_id = torch.zeros((batch_size, 1)).int().cuda()\n",
    "in_id[0] = solo_speaker\n",
    "# for i in range(batch_size):\n",
    "#    in_id[i] = i\n",
    "in_emo = torch.zeros((batch_size, in_audio.shape[1]//16000*15)).int() + 0\n",
    "in_emo = in_emo.cuda()\n",
    "pre_frames = 4\n",
    "in_pre_facial = torch.zeros((batch_size,in_audio.shape[1]//16000*15, 52)).float().cuda()\n",
    "in_pre_facial[:, 0:pre_frames, -1] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 450, 51])\n"
     ]
    }
   ],
   "source": [
    "pred_facial = net(in_pre_facial, in_audio=in_audio, in_id=in_id, in_emo=in_emo).cpu().detach()\n",
    "print(pred_facial.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-12 17:01:01.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.other_tools\u001b[0m:\u001b[36mload_checkpoints\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mload self-pretrained checkpoints for CaMN\u001b[0m\n"
     ]
    }
   ],
   "source": [
    " # load in model\n",
    "model_path = os.path.join(camn_args.root_path, 'datasets/beat_cache/beat_4english_15_141/weights/camn.bin')\n",
    "camn_model = CaMN(camn_args)\n",
    "load_checkpoints(camn_model, camn_args.root_path+camn_args.test_ckpt, camn_args.g_name)\n",
    "camn_model = camn_model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 450, 142]) torch.Size([1, 450, 51]) torch.Size([1, 480000]) torch.Size([1, 1]) torch.Size([1, 450])\n"
     ]
    }
   ],
   "source": [
    "if facial_norm:\n",
    "    in_facial = pred_facial.cuda()\n",
    "else:\n",
    "    in_facial = ((pred_facial - mean_facial)/std_facial).cuda()\n",
    "\n",
    "pre_frames = 4\n",
    "pre_pose = torch.zeros((batch_size, in_facial.shape[1], gesturegen_args.pose_dims + 1)).cuda()\n",
    "pre_pose[:, 0:pre_frames, :-1] = template['pose'][:, 0:pre_frames]\n",
    "pre_pose[:, 0:pre_frames, -1] = 1\n",
    "\n",
    "in_audio = in_audio.reshape(1, -1)\n",
    "\n",
    "print(pre_pose.shape, in_facial.shape, in_audio.shape, in_id.shape, in_emo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir_vec = camn_model(pre_seq=pre_pose, in_audio=in_audio, in_facial=in_facial, in_id=in_id, in_emo=in_emo)\n",
    "out_final = np.array((out_dir_vec.cpu().detach().reshape(-1, camn_args.pose_dims) * std_pose) + mean_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 141)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joints_list import JOINTS_LIST\n",
    "\n",
    "def result2target_vis(template_bvh, bvh_file, res_frames, save_path):\n",
    "    ori_list = JOINTS_LIST[\"beat_joints\"]\n",
    "    target_list = JOINTS_LIST[\"spine_neck_141\"]\n",
    "    file_content_length = 431\n",
    "\n",
    "    template_bvh_path = f\"{camn_args.root_path}/datasets/beat_cache/beat_4english_15_141/test/bvh_rot_vis/{template_bvh}\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    short_name = bvh_file.split(\"\\\\\")[-1][11:]\n",
    "    save_file_path = os.path.join(save_path, f'res_{short_name}')\n",
    "    \n",
    "    write_file = open(save_file_path,'w+')\n",
    "    with open(template_bvh_path,'r') as pose_data_pre:\n",
    "        pose_data_pre_file = pose_data_pre.readlines()\n",
    "        ori_lines = pose_data_pre_file[:file_content_length]\n",
    "        offset_data = np.fromstring(pose_data_pre_file[file_content_length], dtype=float, sep=' ')\n",
    "    write_file.close()\n",
    "\n",
    "    ori_lines[file_content_length-2] = 'Frames: ' + str(res_frames) + '\\n'\n",
    "    ori_lines[file_content_length-1] = 'Frame Time: 0.066667\\n'\n",
    "\n",
    "    write_file = open(os.path.join(save_path, f'res_{short_name}'),'w+')\n",
    "    write_file.writelines(i for i in ori_lines[:file_content_length])    \n",
    "    write_file.close() \n",
    "\n",
    "    with open(save_file_path,'a+') as write_file: \n",
    "        with open(bvh_file, 'r') as pose_data:\n",
    "            data_each_file = []\n",
    "            pose_data_file = pose_data.readlines()\n",
    "            for j, line in enumerate(pose_data_file):\n",
    "                if not j:\n",
    "                    pass\n",
    "                else:          \n",
    "                    data = np.fromstring(line, dtype=float, sep=' ')\n",
    "                    data_rotation = offset_data.copy()   \n",
    "                    for iii, (k, v) in enumerate(target_list.items()): # here is 147 rotations by 3\n",
    "                        data_rotation[ori_list[k][1]-v:ori_list[k][1]] = data[iii*3:iii*3+3]\n",
    "                    data_each_file.append(data_rotation)\n",
    "    \n",
    "        for line_data in data_each_file:\n",
    "            line_data = np.array2string(line_data, max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "            write_file.write(line_data[1:-2]+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file = os.path.join(\"result_pose\",f\"result_raw_gandhi_speech.bvh\")\n",
    "\n",
    "with open(res_file, 'w+') as f_real:\n",
    "    for line_id in range(out_final.shape[0]): #,args.pre_frames, args.pose_length\n",
    "        line_data = np.array2string(out_final[line_id], max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "        f_real.write(line_data[1:-2]+'\\n')  \n",
    "res_frames = out_final.shape[0] - 1\n",
    "result2target_vis(template_bvh, res_file, res_frames, 'result_pose/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_json = f'{template_bvh[:-4]}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from blendshapes import BLENDSHAPE_NAMES\n",
    "\n",
    "def bs2json(facial):\n",
    "    with open(f\"result_pose/res_gandhi_speech.json\", \"w\") as res_json:\n",
    "        new_frames_list = []\n",
    "        time = 0.016666666666666666\n",
    "        for weights in facial:\n",
    "            new_frames_list.append({'weights': weights, 'time': time, 'rotation': []})\n",
    "            time += 1/15\n",
    "        json_new = {\"names\":BLENDSHAPE_NAMES[:-1], \"frames\": new_frames_list}\n",
    "        json.dump(json_new, res_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs2json((in_facial[0].cpu() * std_facial + mean_facial).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Animation in Unreal Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blendshapes import BLENDSHAPE_NAMES\n",
    "def play_audio(out_audio, init_time):\n",
    "    time.sleep(init_time - time.time())\n",
    "    sd.play(out_audio, 16000)\n",
    "    sd.wait()\n",
    "    print(\"Audio finished:\", time.time())\n",
    "\n",
    "def send_udp(out_face, init_time):\n",
    "    #outWeight = np.zeros(52)\n",
    "\n",
    "    ##need to implement get value in\n",
    "    outWeight = out_face\n",
    "\n",
    "    outWeight = outWeight * (outWeight >= 0)\n",
    "\n",
    "    client = udp_client.SimpleUDPClient('127.0.0.1', 5008)\n",
    "    osc_array = outWeight.tolist()\n",
    "    \n",
    "    fps = 15\n",
    "    time.sleep(init_time - time.time())\n",
    "    #start_time = time.time()\n",
    "    for i in range(len(osc_array)):\n",
    "        #print(out_face[i].shape)\n",
    "        for j, out in enumerate(osc_array[i]):\n",
    "            client.send_message('/' + str(BLENDSHAPE_NAMES[j]), out)\n",
    "\n",
    "        elpased_time = time.time() - init_time\n",
    "        sleep_time = 1.0/fps * (i+1) - elpased_time\n",
    "        if sleep_time > 0:\n",
    "            time.sleep(sleep_time)\n",
    "        #start_time = time.time()\n",
    "    print(\"Facial finished:\", time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_id[0] = 14\n",
    "pred_facial = net(in_pre_facial, in_audio=in_audio, in_id=in_id, in_emo=in_emo).cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio finished: 1718242266.379503\n",
      "Facial finished: 1718242266.4154744\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "init_time = time.time() + 1\n",
    "\n",
    "limit_sec = 10\n",
    "\n",
    "#facial_ue = template['facial'] * std_facial + mean_facial\n",
    "facial_ue = pred_facial\n",
    "facial_ue[:,:,8:24] = 0\n",
    "# blinking animation\n",
    "blink_interval = 5 # in seconds\n",
    "for i in range(blink_interval*15,limit_sec*15,blink_interval*15):\n",
    "    blink_duration = 2 # in frames\n",
    "    for j in range(i-blink_duration,i): # start blinking\n",
    "        facial_ue[:,j,8:10] = 1-(i-j)/blink_duration\n",
    "    for j in range(i, i+blink_duration):\n",
    "        facial_ue[:,j,8:10] = 1-(j-i)/blink_duration\n",
    "\n",
    "udp_thread = threading.Thread(target=send_udp, args=(facial_ue[0, 0:limit_sec*15],init_time))\n",
    "udp_thread.daemon = True  # Set the thread as a daemon to allow it to exit when the main program exits\n",
    "\n",
    "# audio_ue = template['audio']\n",
    "audio_ue = out_audio\n",
    "audio_thread = threading.Thread(target=play_audio, args=(audio_ue[0, 0:limit_sec*16000],init_time-0.3))\n",
    "audio_thread.daemon = True\n",
    "\n",
    "udp_thread.start()\n",
    "audio_thread.start()\n",
    "\n",
    "udp_thread.join()\n",
    "audio_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = udp_client.SimpleUDPClient('127.0.0.1', 5008)\n",
    "default_face = np.zeros((51,))\n",
    "default_face[8] = 1\n",
    "for j, out in enumerate(default_face):\n",
    "            client.send_message('/' + str(BLENDSHAPE_NAMES[j]), out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
