{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonosc import udp_client\n",
    "import time\n",
    "import sounddevice as sd\n",
    "import torch\n",
    "from dataloaders.beat import CustomDataset\n",
    "from dataloaders.build_vocab import Vocab\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "camn_config_file = open(\"camn_config.obj\", 'rb') \n",
    "args = pickle.load(camn_config_file)\n",
    "args.batch_size = 16\n",
    "\n",
    "mean_facial = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.facial_rep}/json_mean.npy\")).float()\n",
    "std_facial = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.facial_rep}/json_std.npy\")).float()\n",
    "mean_audio = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.audio_rep}/npy_mean.npy\")).float()\n",
    "std_audio = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.audio_rep}/npy_std.npy\")).float()\n",
    "mean_pose = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.pose_rep}/bvh_mean.npy\")).float()\n",
    "std_pose = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.pose_rep}/bvh_std.npy\")).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomDataset(args, \"test\")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=1,  \n",
    "    shuffle=False,  \n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for its, data in enumerate(test_loader):\n",
    "    if its == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = data['pose']\n",
    "audio = data['audio']\n",
    "facial = data['facial']\n",
    "id = data[\"id\"]\n",
    "word = data[\"word\"]\n",
    "emo = data[\"emo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-12 04:10:14.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.other_tools\u001b[0m:\u001b[36mload_checkpoints\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mload self-pretrained checkpoints for CaMN\u001b[0m\n"
     ]
    }
   ],
   "source": [
    " # load in model\n",
    "import os\n",
    "from utils.other_tools import load_checkpoints\n",
    "from models.camn import CaMN\n",
    "model_path = os.path.join(args.root_path, 'datasets/beat_cache/beat_4english_15_141/weights/camn.bin')\n",
    "camn_model = CaMN(args)\n",
    "load_checkpoints(camn_model, args.root_path+args.test_ckpt, args.g_name)\n",
    "camn_model = camn_model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_pose = pose.cuda()\n",
    "in_audio = audio.cuda()\n",
    "in_facial = facial.cuda()\n",
    "in_id = id.cuda()\n",
    "in_emo = emo.cuda()\n",
    "\n",
    "pre_frames = 4\n",
    "pre_pose = tar_pose.new_zeros((tar_pose.shape[0], tar_pose.shape[1], tar_pose.shape[2] + 1)).cuda()\n",
    "pre_pose[:, 0:pre_frames, :-1] = tar_pose[:, 0:pre_frames]\n",
    "pre_pose[:, 0:pre_frames, -1] = 1\n",
    "\n",
    "in_audio = in_audio.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir_vec = camn_model(pre_seq=pre_pose, in_audio=in_audio, in_facial=in_facial, in_id=in_id, in_emo=in_emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_final = np.array((out_dir_vec.cpu().detach().reshape(-1, args.pose_dims) * std_pose) + mean_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 141)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_demo = args.root_path + args.test_data_path + f\"{args.pose_rep}_vis/\"\n",
    "test_seq_list = os.listdir(test_demo)\n",
    "test_seq_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"result_pose/result_raw_{test_seq_list[its]}\", 'w+') as f_real:\n",
    "    for line_id in range(out_final.shape[0]): #,args.pre_frames, args.pose_length\n",
    "        line_data = np.array2string(out_final[line_id], max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "        f_real.write(line_data[1:-2]+'\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
