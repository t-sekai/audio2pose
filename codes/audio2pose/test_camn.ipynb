{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonosc import udp_client\n",
    "import time\n",
    "import sounddevice as sd\n",
    "import torch\n",
    "from dataloaders.beat import CustomDataset\n",
    "from dataloaders.build_vocab import Vocab\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from utils.other_tools import load_checkpoints\n",
    "from models.camn import CaMN\n",
    "\n",
    "camn_config_file = open(\"camn_config.obj\", 'rb') \n",
    "gesturegen_config_file = open(\"gesturegen_config.obj\", 'rb')\n",
    "\n",
    "gesturegen_args = pickle.load(gesturegen_config_file)\n",
    "camn_args = pickle.load(camn_config_file)\n",
    "\n",
    "mean_facial = torch.from_numpy(np.load(camn_args.root_path+camn_args.mean_pose_path+f\"{camn_args.facial_rep}/json_mean.npy\")).float()\n",
    "std_facial = torch.from_numpy(np.load(camn_args.root_path+camn_args.mean_pose_path+f\"{camn_args.facial_rep}/json_std.npy\")).float()\n",
    "mean_audio = torch.from_numpy(np.load(camn_args.root_path+camn_args.mean_pose_path+f\"{camn_args.audio_rep}/npy_mean.npy\")).float()\n",
    "std_audio = torch.from_numpy(np.load(camn_args.root_path+camn_args.mean_pose_path+f\"{camn_args.audio_rep}/npy_std.npy\")).float()\n",
    "mean_pose = torch.from_numpy(np.load(camn_args.root_path+camn_args.mean_pose_path+f\"{camn_args.pose_rep}/bvh_mean.npy\")).float()\n",
    "std_pose = torch.from_numpy(np.load(camn_args.root_path+camn_args.mean_pose_path+f\"{camn_args.pose_rep}/bvh_std.npy\")).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18_daiki_0_103_a.bvh\n"
     ]
    }
   ],
   "source": [
    "test_data = CustomDataset(camn_args, \"test\")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=1,  \n",
    "    shuffle=False,  \n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "batch_size = 1\n",
    "solo_speaker = 17\n",
    "\n",
    "for its, template in enumerate(test_loader):\n",
    "    if template['id'][0] == solo_speaker:\n",
    "        break\n",
    "\n",
    "test_demo = camn_args.root_path + camn_args.test_data_path + f\"{camn_args.pose_rep}_vis/\"\n",
    "test_seq_list = os.listdir(test_demo)\n",
    "test_seq_list.sort()\n",
    "\n",
    "template_file = test_seq_list[its]\n",
    "print(template_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " # load in facial model\n",
    "from scripts.MulticontextNet import GestureGen\n",
    "model_path = 'tmp/multicontextnet-no-text.pth'\n",
    "net = GestureGen(gesturegen_args)\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "net = net.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sample rate: 22050\n"
     ]
    }
   ],
   "source": [
    "# load in test_audio\n",
    "import librosa\n",
    "test_audio_file = 'test_audio/gandhi-speech.wav'\n",
    "test_audio_raw, sr = librosa.load(test_audio_file, sr=None) # np array\n",
    "test_audio = librosa.resample(test_audio_raw, orig_sr=sr, target_sr=16000)#test_audio_raw[::sr//16000] # convert to 16khz\n",
    "print('Original sample rate:', sr)\n",
    "out_audio = torch.from_numpy(test_audio).unsqueeze(0)\n",
    "audio = (out_audio - mean_audio) / std_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio finished: 1718228446.8094735\n"
     ]
    }
   ],
   "source": [
    "limit_sec = 10\n",
    "sd.play(test_audio[0:limit_sec*16000], 16000)\n",
    "sd.wait()\n",
    "print(\"Audio finished:\", time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_audio = audio.expand(batch_size, -1).cuda()\n",
    "in_id = torch.zeros((batch_size, 1)).int().cuda()\n",
    "in_id[0] = solo_speaker\n",
    "# for i in range(batch_size):\n",
    "#    in_id[i] = i\n",
    "in_emo = torch.zeros((batch_size, in_audio.shape[1]//16000*15)).int() + 0\n",
    "in_emo = in_emo.cuda()\n",
    "pre_frames = 4\n",
    "in_pre_facial = torch.zeros((batch_size,in_audio.shape[1]//16000*15, 52)).float().cuda()\n",
    "in_pre_facial[:, 0:pre_frames, :-1] = template['facial'][:, 0:pre_frames]\n",
    "in_pre_facial[:, 0:pre_frames, -1] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5580, 51])\n"
     ]
    }
   ],
   "source": [
    "pred_facial = net(in_pre_facial, in_audio=in_audio, in_id=in_id, in_emo=in_emo).cpu().detach()\n",
    "print(pred_facial.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-12 14:40:57.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.other_tools\u001b[0m:\u001b[36mload_checkpoints\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mload self-pretrained checkpoints for CaMN\u001b[0m\n"
     ]
    }
   ],
   "source": [
    " # load in model\n",
    "model_path = os.path.join(camn_args.root_path, 'datasets/beat_cache/beat_4english_15_141/weights/camn.bin')\n",
    "camn_model = CaMN(camn_args)\n",
    "load_checkpoints(camn_model, camn_args.root_path+camn_args.test_ckpt, camn_args.g_name)\n",
    "camn_model = camn_model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5580, 142]) torch.Size([1, 5580, 51]) torch.Size([1, 5954075]) torch.Size([1, 1]) torch.Size([1, 5580])\n"
     ]
    }
   ],
   "source": [
    "in_facial = pred_facial.cuda()\n",
    "\n",
    "pre_frames = 4\n",
    "pre_pose = torch.zeros((batch_size, in_facial.shape[1], gesturegen_args.pose_dims + 1)).cuda()\n",
    "pre_pose[:, 0:pre_frames, :-1] = template['pose'][:, 0:pre_frames]\n",
    "pre_pose[:, 0:pre_frames, -1] = 1\n",
    "\n",
    "in_audio = in_audio.reshape(1, -1)\n",
    "\n",
    "print(pre_pose.shape, in_facial.shape, in_audio.shape, in_id.shape, in_emo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir_vec = camn_model(pre_seq=pre_pose, in_audio=in_audio, in_facial=in_facial, in_id=in_id, in_emo=in_emo)\n",
    "out_final = np.array((out_dir_vec.cpu().detach().reshape(-1, camn_args.pose_dims) * std_pose) + mean_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5580, 141)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joints_list import JOINTS_LIST\n",
    "\n",
    "def result2target_vis(template_file, bvh_file, res_frames, save_path):\n",
    "    ori_list = JOINTS_LIST[\"beat_joints\"]\n",
    "    target_list = JOINTS_LIST[\"spine_neck_141\"]\n",
    "    file_content_length = 431\n",
    "\n",
    "    template_file_path = f\"{camn_args.root_path}/datasets/beat_cache/beat_4english_15_141/test/bvh_rot_vis/{template_file}\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    short_name = bvh_file.split(\"\\\\\")[-1][11:]\n",
    "    save_file_path = os.path.join(save_path, f'res_{short_name}')\n",
    "    \n",
    "    write_file = open(save_file_path,'w+')\n",
    "    with open(template_file_path,'r') as pose_data_pre:\n",
    "        pose_data_pre_file = pose_data_pre.readlines()\n",
    "        ori_lines = pose_data_pre_file[:file_content_length]\n",
    "        offset_data = np.fromstring(pose_data_pre_file[file_content_length], dtype=float, sep=' ')\n",
    "    write_file.close()\n",
    "\n",
    "    ori_lines[file_content_length-2] = 'Frames: ' + str(res_frames) + '\\n'\n",
    "\n",
    "    write_file = open(os.path.join(save_path, f'res_{short_name}'),'w+')\n",
    "    write_file.writelines(i for i in ori_lines[:file_content_length])    \n",
    "    write_file.close() \n",
    "\n",
    "    with open(save_file_path,'a+') as write_file: \n",
    "        with open(bvh_file, 'r') as pose_data:\n",
    "            data_each_file = []\n",
    "            pose_data_file = pose_data.readlines()\n",
    "            for j, line in enumerate(pose_data_file):\n",
    "                if not j:\n",
    "                    pass\n",
    "                else:          \n",
    "                    data = np.fromstring(line, dtype=float, sep=' ')\n",
    "                    data_rotation = offset_data.copy()   \n",
    "                    for iii, (k, v) in enumerate(target_list.items()): # here is 147 rotations by 3\n",
    "                        data_rotation[ori_list[k][1]-v:ori_list[k][1]] = data[iii*3:iii*3+3]\n",
    "                    data_each_file.append(data_rotation)\n",
    "    \n",
    "        for line_data in data_each_file:\n",
    "            line_data = np.array2string(line_data, max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "            write_file.write(line_data[1:-2]+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file = os.path.join(\"result_pose\",f\"result_raw_ghandhi_speech.bvh\")\n",
    "\n",
    "with open(res_file, 'w+') as f_real:\n",
    "    for line_id in range(out_final.shape[0]): #,args.pre_frames, args.pose_length\n",
    "        line_data = np.array2string(out_final[line_id], max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "        f_real.write(line_data[1:-2]+'\\n')  \n",
    "res_frames = out_final.shape[0] - 1\n",
    "result2target_vis(template_file, res_file, res_frames, 'result_pose/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
