{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataloaders.beat import CustomDataset\n",
    "from dataloaders.build_vocab import Vocab\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "config_file = open(\"gesturegen_config.obj\", 'rb') \n",
    "args = pickle.load(config_file)\n",
    "args.batch_size = 16\n",
    "\n",
    "mean_facial = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.facial_rep}/json_mean.npy\"))\n",
    "std_facial = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.facial_rep}/json_std.npy\"))\n",
    "mean_audio = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.audio_rep}/npy_mean.npy\"))\n",
    "std_audio = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.audio_rep}/npy_std.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(args, \"train\")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=args.batch_size,  \n",
    "    shuffle=True,  \n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14857"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = CustomDataset(args, \"val\")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data, \n",
    "    batch_size=args.batch_size,  \n",
    "    shuffle=True,  \n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2972"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.4506e-09) tensor(0.9524) tensor(0.1426) tensor(0.1631)\n",
      "tensor(-6.0584) tensor(7.1772) tensor(-0.0174) tensor(1.1267)\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_loader))\n",
    "facial = data['facial'] * std_facial.float() + mean_facial.float()\n",
    "to_flame = torch.from_numpy(np.load('mat_final.npy'))\n",
    "facial_vertex = facial @ to_flame\n",
    "print(facial.min(), facial.max(), facial.mean(), facial.std())\n",
    "print(facial_vertex.min(), facial_vertex.max(), facial_vertex.mean(), facial_vertex.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test A2BS SimpleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scripts.Dataset import a2bsDataset\n",
    "from scripts.MulticontextNet import GestureGen\n",
    "#import wandb\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project = \"testing\"\n",
    "# group = \"facegenerator\"\n",
    "# name = \"test\"\n",
    "# dataset_name = \"beat\"\n",
    "# entity = \"hm_gesture\"\n",
    "# run = wandb.init(\n",
    "#         project=\"testing\",\n",
    "#         group=\"facegenerator\",\n",
    "#         name= f\"{name}-{dataset_name}-{str(uuid.uuid4())[:8]}\",\n",
    "#         id=str(uuid.uuid4()),\n",
    "#         entity=entity,\n",
    "#     )\n",
    "# wandb.run.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'ckpt_model/multicontextnet-all-vertex-2024/multicontextnet-300.pth'\n",
    "net = GestureGen(args).cuda()\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "optimizer = torch.optim.Adam( net.parameters(), lr=1e-4)#, weight_decay=1e-5)\n",
    "train_target_loss = []\n",
    "train_expressive_loss = []\n",
    "train_smooth_loss = []\n",
    "train_mse_loss = []\n",
    "val_target_loss = []\n",
    "val_expressive_loss = []\n",
    "val_smooth_loss = []\n",
    "val_mse_loss = []\n",
    "\n",
    "def plot_train_val_loss():\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(10, 3))\n",
    "    axs[0].plot(train_target_loss, 'r-')\n",
    "    axs[0].set_title('Target Loss')\n",
    "    axs[1].plot(train_expressive_loss, 'p-')\n",
    "    axs[1].set_title('Expressive Loss')\n",
    "    axs[2].plot(train_smooth_loss, 'g-')\n",
    "    axs[2].set_title('Smooth Loss')\n",
    "    axs[3].plot(train_mse_loss, 'b-')\n",
    "    axs[3].set_title('MSE Loss')\n",
    "    fig.suptitle('Training Iterations', fontsize = 16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(10, 3))\n",
    "    axs[0].plot(val_target_loss, 'r-')\n",
    "    axs[0].set_title('Target Loss')\n",
    "    axs[1].plot(val_expressive_loss, 'p-')\n",
    "    axs[1].set_title('Expressive Loss')\n",
    "    axs[2].plot(val_smooth_loss, 'g-')\n",
    "    axs[2].set_title('Smooth Loss')\n",
    "    axs[3].plot(val_mse_loss, 'b-')\n",
    "    axs[3].set_title('MSE Loss')\n",
    "    fig.suptitle('Validation Iterations', fontsize = 16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237714\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "data = next(iter(train_loader))\n",
    "in_audio = data['audio']\n",
    "facial = data['facial']\n",
    "in_id = data[\"id\"]\n",
    "in_word = data[\"word\"]\n",
    "in_emo = data[\"emo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expressive_loss_function(output, target): # max squared error over blendshape for each frame, then take the mean\n",
    "    loss = torch.mean(torch.max((output - target) ** 2, dim=-1).values)\n",
    "    return loss\n",
    "# a = torch.tensor([[[1,2,3],[1,2,3]],[[4,5,6],[4,5,6]]]).float()\n",
    "# b = torch.tensor([[[3,3,4],[2,3,4]],[[5,6,7],[5,6,7]]]).float()\n",
    "# print(a.shape)\n",
    "# max_square_error(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "num_epochs = 700\n",
    "log_period = 200\n",
    "val_period = 600\n",
    "val_size = 25\n",
    "to_flame = torch.from_numpy(np.load('mat_final.npy')).cuda()\n",
    "target_loss_function = torch.nn.HuberLoss()\n",
    "smooth_loss_function = torch.nn.CosineSimilarity(dim=2)\n",
    "mse_loss_function = torch.nn.MSELoss()\n",
    "target_weight = 1.5\n",
    "expressive_weight = 0.5\n",
    "smooth_weight = 0.5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for it, data in enumerate(tqdm(train_loader)):\n",
    "        net.train()\n",
    "        in_audio = data['audio']\n",
    "        facial = data['facial']\n",
    "        in_id = data[\"id\"]\n",
    "        in_word = data[\"word\"]\n",
    "        in_emo = data[\"emo\"]\n",
    "\n",
    "        in_audio = in_audio.cuda()\n",
    "        facial = facial.cuda()\n",
    "        in_id = in_id.cuda()\n",
    "        in_word = in_word.cuda()\n",
    "        in_emo = in_emo.cuda()\n",
    "\n",
    "        pre_frames = 4\n",
    "        in_pre_face = facial.new_zeros((facial.shape[0], facial.shape[1], facial.shape[2] + 1)).cuda()\n",
    "        in_pre_face[:, 0:pre_frames, :-1] = facial[:, 0:pre_frames]\n",
    "        in_pre_face[:, 0:pre_frames, -1] = 1 \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out_face = net(in_pre_face,in_audio=in_audio,in_text=in_word, in_id=in_id, in_emo=in_emo)\n",
    "        target_loss = target_loss_function(out_face@to_flame, facial@to_flame) + target_loss_function(out_face[:,:,6:14], facial[:,:,6:14]) # to account for eye movement\n",
    "        expressive_loss = expressive_loss_function(out_face, facial)\n",
    "        smooth_loss = 1 - smooth_loss_function(out_face[:,:-1,:], out_face[:,1:,:]).mean()\n",
    "        loss = target_weight * target_loss  + smooth_weight * smooth_loss# + expressive_weight * expressive_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_target_loss.append(target_loss.item())\n",
    "        train_expressive_loss.append(expressive_loss.item())\n",
    "        train_smooth_loss.append(smooth_loss.item())\n",
    "        train_mse_loss.append(mse_loss_function(out_face.cpu()*std_facial+mean_facial, facial.cpu()*std_facial+mean_facial).item())\n",
    "        \n",
    "        #logging\n",
    "        #if it % log_period == 0:\n",
    "        #    print(f'[{epoch}][{it}/{len(train_loader)}]: [train] [target loss]: {train_target_loss[-1]} [exp loss]: {train_expressive_loss[-1]} [smooth loss]: {train_smooth_loss[-1]} [mse]: {train_mse_loss[-1]}')\n",
    "        \n",
    "        if it % val_period == 0:\n",
    "            net.eval()\n",
    "            val_target_loss_st = []\n",
    "            val_expressive_loss_st = []\n",
    "            val_smooth_loss_st = []\n",
    "            val_mse_loss_st = []\n",
    "            val_cnt = 0\n",
    "\n",
    "            val_loader = torch.utils.data.DataLoader(\n",
    "                val_data, \n",
    "                batch_size=args.batch_size,  \n",
    "                shuffle=True,  \n",
    "                drop_last=True,\n",
    "            )\n",
    "            \n",
    "            for _, data in enumerate(val_loader):\n",
    "                in_audio = data['audio']\n",
    "                facial = data['facial']\n",
    "                in_id = data[\"id\"]\n",
    "                in_word = data[\"word\"]\n",
    "                in_emo = data[\"emo\"]\n",
    "\n",
    "                in_audio = in_audio.cuda()\n",
    "                facial = facial.cuda()\n",
    "                in_id = in_id.cuda()\n",
    "                in_word = in_word.cuda()\n",
    "                in_emo = in_emo.cuda()\n",
    "\n",
    "                pre_frames = 4\n",
    "                in_pre_face = facial.new_zeros((facial.shape[0], facial.shape[1], facial.shape[2] + 1)).cuda()\n",
    "                in_pre_face[:, 0:pre_frames, :-1] = facial[:, 0:pre_frames]\n",
    "                in_pre_face[:, 0:pre_frames, -1] = 1 \n",
    "\n",
    "                out_face = net(in_pre_face,in_audio=in_audio,in_text=in_word, in_id=in_id, in_emo=in_emo)\n",
    "                target_loss = target_loss_function(out_face@to_flame, facial@to_flame) + target_loss_function(out_face[:,:,6:14], facial[:,:,6:14])\n",
    "                expressive_loss = expressive_loss_function(out_face, facial)\n",
    "                smooth_loss = 1 - smooth_loss_function(out_face[:,:-1,:], out_face[:,1:,:]).mean()\n",
    "\n",
    "                val_target_loss_st.append(target_loss.item())\n",
    "                val_expressive_loss_st.append(expressive_loss.item())\n",
    "                val_smooth_loss_st.append(smooth_loss.item())\n",
    "                val_mse_loss_st.append(mse_loss_function(out_face.cpu()*std_facial+mean_facial, facial.cpu()*std_facial+mean_facial).item())\n",
    "                \n",
    "                \n",
    "                val_cnt += 1\n",
    "                if val_cnt >= val_size:\n",
    "                    break\n",
    "            \n",
    "            val_target_loss.append(np.average(val_target_loss_st))\n",
    "            val_expressive_loss.append(np.average(val_expressive_loss_st))\n",
    "            val_smooth_loss.append(np.average(val_smooth_loss_st))\n",
    "            val_mse_loss.append(np.average(val_mse_loss_st))\n",
    "            #print(f'[{epoch}][{it}/{len(train_loader)}]: [val] [target loss]: {val_target_loss[-1]} [exp loss]: {val_expressive_loss[-1]} [smooth loss]: {val_smooth_loss[-1]} [mse]: {val_mse_loss[-1]}')\n",
    "    plot_train_val_loss()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'ckpt_model/multicontextnet100.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonosc import udp_client\n",
    "import time\n",
    "import sounddevice as sd\n",
    "import torch\n",
    "from dataloaders.beat import CustomDataset\n",
    "from dataloaders.build_vocab import Vocab\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "config_file = open(\"gesturegen_config.obj\", 'rb') \n",
    "args = pickle.load(config_file)\n",
    "args.batch_size = 16\n",
    "\n",
    "mean_facial = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.facial_rep}/json_mean.npy\")).float()\n",
    "std_facial = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.facial_rep}/json_std.npy\")).float()\n",
    "mean_audio = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.audio_rep}/npy_mean.npy\")).float()\n",
    "std_audio = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.audio_rep}/npy_std.npy\")).float()\n",
    "to_flame = torch.from_numpy(np.load('mat_final.npy')).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomDataset(args, \"test\")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=1,  \n",
    "    shuffle=True,  \n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = data['audio']\n",
    "facial = data['facial']\n",
    "id = data[\"id\"]\n",
    "word = data[\"word\"]\n",
    "emo = data[\"emo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_facial = facial * std_facial + mean_facial\n",
    "out_audio = audio * std_audio + mean_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.4506e-09) tensor(0.9209)\n",
      "tensor(0.1671) tensor(0.1659)\n"
     ]
    }
   ],
   "source": [
    "# Try playing the audio, which is at 16KHZ\n",
    "print(out_facial.min(), out_facial.max())\n",
    "print(out_facial.std(), out_facial.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1072000])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend =  [\n",
    "        \"browDownLeft\",\n",
    "        \"browDownRight\",\n",
    "        \"browInnerUp\",\n",
    "        \"browOuterUpLeft\",\n",
    "        \"browOuterUpRight\",\n",
    "        \"cheekPuff\",\n",
    "        \"cheekSquintLeft\",\n",
    "        \"cheekSquintRight\",\n",
    "        \"eyeBlinkLeft\",\n",
    "        \"eyeBlinkRight\",\n",
    "        \"eyeLookDownLeft\",\n",
    "        \"eyeLookDownRight\",\n",
    "        \"eyeLookInLeft\",\n",
    "        \"eyeLookInRight\",\n",
    "        \"eyeLookOutLeft\",\n",
    "        \"eyeLookOutRight\",\n",
    "        \"eyeLookUpLeft\",\n",
    "        \"eyeLookUpRight\",\n",
    "        \"eyeSquintLeft\",\n",
    "        \"eyeSquintRight\",\n",
    "        \"eyeWideLeft\",\n",
    "        \"eyeWideRight\",\n",
    "        \"jawForward\",\n",
    "        \"jawLeft\",\n",
    "        \"jawOpen\",\n",
    "        \"jawRight\",\n",
    "        \"mouthClose\",\n",
    "        \"mouthDimpleLeft\",\n",
    "        \"mouthDimpleRight\",\n",
    "        \"mouthFrownLeft\",\n",
    "        \"mouthFrownRight\",\n",
    "        \"mouthFunnel\",\n",
    "        \"mouthLeft\",\n",
    "        \"mouthLowerDownLeft\",\n",
    "        \"mouthLowerDownRight\",\n",
    "        \"mouthPressLeft\",\n",
    "        \"mouthPressRight\",\n",
    "        \"mouthPucker\",\n",
    "        \"mouthRight\",\n",
    "        \"mouthRollLower\",\n",
    "        \"mouthRollUpper\",\n",
    "        \"mouthShrugLower\",\n",
    "        \"mouthShrugUpper\",\n",
    "        \"mouthSmileLeft\",\n",
    "        \"mouthSmileRight\",\n",
    "        \"mouthStretchLeft\",\n",
    "        \"mouthStretchRight\",\n",
    "        \"mouthUpperUpLeft\",\n",
    "        \"mouthUpperUpRight\",\n",
    "        \"noseSneerLeft\",\n",
    "        \"noseSneerRight\",\n",
    "        \"tongueOut\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_audio(out_audio, init_time):\n",
    "    time.sleep(init_time - time.time())\n",
    "    sd.play(out_audio, 16000)\n",
    "    sd.wait()\n",
    "    print(\"Audio finished:\", time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def send_udp(out_face, init_time):\n",
    "    #outWeight = np.zeros(52)\n",
    "\n",
    "    ##need to implement get value in\n",
    "    outWeight = out_face\n",
    "\n",
    "    outWeight = outWeight * (outWeight >= 0)\n",
    "\n",
    "    client = udp_client.SimpleUDPClient('127.0.0.1', 5008)\n",
    "    osc_array = outWeight.tolist()\n",
    "    \n",
    "    fps = 15\n",
    "    time.sleep(init_time - time.time())\n",
    "    #start_time = time.time()\n",
    "    for i in range(len(osc_array)):\n",
    "        #print(out_face[i].shape)\n",
    "        for j, out in enumerate(osc_array[i]):\n",
    "            client.send_message('/' + str(blend[j]), out)\n",
    "\n",
    "        elpased_time = time.time() - init_time\n",
    "        sleep_time = 1.0/fps * (i+1) - elpased_time\n",
    "        if sleep_time > 0:\n",
    "            time.sleep(sleep_time)\n",
    "        #start_time = time.time()\n",
    "    print(\"Facial finished:\", time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio finished: 1717109722.657436\n",
      "Facial finished: 1717109722.8734372\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "init_time = time.time() + 1\n",
    "\n",
    "limit_sec = 20\n",
    "\n",
    "udp_thread = threading.Thread(target=send_udp, args=(out_facial[0, 0:limit_sec*15],init_time))\n",
    "udp_thread.daemon = True  # Set the thread as a daemon to allow it to exit when the main program exits\n",
    "\n",
    "audio_thread = threading.Thread(target=play_audio, args=(out_audio[0, 0:limit_sec*16000],init_time-0.3))\n",
    "audio_thread.daemon = True\n",
    "\n",
    "udp_thread.start()\n",
    "audio_thread.start()\n",
    "\n",
    "udp_thread.join()\n",
    "audio_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.0 67.0\n"
     ]
    }
   ],
   "source": [
    "print(len(out_audio[0])/16000, len(out_facial[0])/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    " # load in model\n",
    "from scripts.MulticontextNet import GestureGen\n",
    "model_path = 'ckpt_model/multicontextnet-all-vertex-2024/multicontextnet-300.pth'\n",
    "#model_path = 'ckpt_model/multicontextnet100.pth'\n",
    "net = GestureGen(args)\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "net = net.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_audio = audio.cuda()\n",
    "in_facial = facial.cuda() * std_facial.float().cuda() + mean_facial.float().cuda()\n",
    "in_id = id.cuda()\n",
    "in_word = word.cuda()\n",
    "#in_emo = emo.cuda()\n",
    "in_emo = torch.zeros_like(emo) + 1\n",
    "in_emo = in_emo.cuda()\n",
    "pre_frames = 4\n",
    "in_pre_facial = in_facial.new_zeros((in_facial.shape[0], in_facial.shape[1], in_facial.shape[2] + 1)).cuda() \n",
    "in_pre_facial[:, 0:pre_frames, :-1] = in_facial[:, 0:pre_frames]\n",
    "in_pre_facial[:, 0:pre_frames, -1] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_facial = net(in_pre_facial, in_audio=in_audio, in_text=in_word, in_id=in_id, in_emo=in_emo).cpu().detach()\n",
    "#pred_facial = pred_facial * std_facial + mean_facial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.4506e-09) tensor(0.9209)\n",
      "tensor(0.1671) tensor(0.1659)\n",
      "tensor(-0.7328) tensor(1.2204)\n",
      "tensor(0.1933) tensor(0.1581)\n"
     ]
    }
   ],
   "source": [
    "print(in_facial.cpu().min(), in_facial.cpu().max())\n",
    "print(in_facial.cpu().std(), in_facial.cpu().mean())\n",
    "print(pred_facial.min(), pred_facial.max())\n",
    "print(pred_facial.std(), pred_facial.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio finished: 1717110581.9901443\n",
      "Facial finished: 1717110582.2231467\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "init_time = time.time() + 1\n",
    "\n",
    "limit_sec = 20\n",
    "\n",
    "udp_thread = threading.Thread(target=send_udp, args=(pred_facial[0,0:limit_sec*15],init_time))\n",
    "udp_thread.daemon = True  # Set the thread as a daemon to allow it to exit when the main program exits\n",
    "\n",
    "audio_thread = threading.Thread(target=play_audio, args=(out_audio[0,0:limit_sec*16000],init_time-0.3))\n",
    "audio_thread.daemon = True\n",
    "\n",
    "udp_thread.start()\n",
    "audio_thread.start()\n",
    "\n",
    "udp_thread.join()\n",
    "audio_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1283)\n",
      "tensor(0.0159)\n",
      "tensor(0.3069)\n"
     ]
    }
   ],
   "source": [
    "print(expressive_loss_function(pred_facial, out_facial))\n",
    "print(torch.nn.functional.mse_loss(pred_facial, out_facial))\n",
    "print(torch.nn.functional.mse_loss(pred_facial@to_flame, out_facial@to_flame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1502)\n",
      "tensor(0.0188)\n",
      "tensor(0.3199)\n"
     ]
    }
   ],
   "source": [
    "print(expressive_loss_function(pred_facial, out_facial))\n",
    "print(torch.nn.functional.mse_loss(pred_facial, out_facial))\n",
    "print(torch.nn.functional.mse_loss(pred_facial@to_flame, out_facial@to_flame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_facial.shape)\n",
    "print(out_facial.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_facial[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_facial[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
