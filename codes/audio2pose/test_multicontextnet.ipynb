{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders.beat import CustomDataset\n",
    "from dataloaders.build_vocab import Vocab\n",
    "import pickle\n",
    "import numpy as np\n",
    "from utils import config\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scripts.MulticontextNet import GestureGen, ConvDiscriminator\n",
    "from scripts.Logger import Logger\n",
    "import wandb\n",
    "import random\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "import trimesh\n",
    "from blendshapes import BLENDSHAPE_NAMES\n",
    "from time import time\n",
    "\n",
    "config_file = open(\"gesturegen_config.obj\", 'rb') \n",
    "args = pickle.load(config_file)\n",
    "args.batch_size = 16\n",
    "args.continue_training = False\n",
    "args.no_adv_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, args, device, train_data, val_data, model, d_model, logger):\n",
    "        # Set up data loading\n",
    "        self.mean_facial = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.facial_rep}/json_mean.npy\")).float()\n",
    "        self.std_facial = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.facial_rep}/json_std.npy\")).float()\n",
    "        self.mean_audio = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.audio_rep}/npy_mean.npy\")).float()\n",
    "        self.std_audio = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.audio_rep}/npy_std.npy\")).float()\n",
    "\n",
    "        self.batch_size = args.batch_size\n",
    "        self.train_data = train_data\n",
    "        self.train_loader = torch.utils.data.DataLoader(\n",
    "            train_data, \n",
    "            batch_size=self.batch_size,  \n",
    "            shuffle=True,  \n",
    "            drop_last=True,\n",
    "        )\n",
    "        self.val_data = val_data\n",
    "\n",
    "        # Set up model and loss functions\n",
    "        self.no_text = args.no_text\n",
    "        self.model = model.to(device) # generative model\n",
    "        self.d_model = d_model.to(device) # discriminator\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        self.d_optimizer = torch.optim.Adam(self.d_model.parameters(), lr=1e-4)\n",
    "        self.target_loss_function = torch.nn.HuberLoss()\n",
    "        self.smooth_loss_function = torch.nn.CosineSimilarity(dim=2)\n",
    "        self.mse_loss_function = torch.nn.MSELoss()\n",
    "\n",
    "        # Set up blendshape to flame parameters\n",
    "        self.bs_to_flame = torch.from_numpy(np.load('mat_final.npy')).to(device)\n",
    "        self.flame_to_bs = self.bs_to_flame.pinverse()\n",
    "        self.predict_flame = args.predict_flame\n",
    "        self.bs2vertices = args.bs2vertices\n",
    "        self.normalize_face = args.normalize_face\n",
    "        self.pre_frames = args.pre_frames\n",
    "\n",
    "        # Set up blendshape to vertices\n",
    "        if self.bs2vertices:\n",
    "            self.V_factor = 100\n",
    "            self.V_basis = torch.tensor(trimesh.load('bs/Basis.obj').vertices, dtype=torch.float32) * self.V_factor\n",
    "            self.V_bs = torch.stack([torch.tensor(trimesh.load(f'bs/exp/{bs_name}.obj').vertices, dtype=torch.float32) for bs_name in BLENDSHAPE_NAMES[:51]]) * self.V_factor\n",
    "            self.V_deltas = (self.V_bs - self.V_basis.unsqueeze(0)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "        # Set up training/validation parameters\n",
    "        self.epochs = args.epochs\n",
    "        self.target_weight = args.target_weight\n",
    "        self.smooth_weight = args.smooth_weight\n",
    "        self.expressive_weight = args.expressive_weight\n",
    "        self.adv_weight = args.adv_weight\n",
    "        self.val_size = args.val_size\n",
    "        self.log_period = args.log_period\n",
    "        self.val_period = args.val_period\n",
    "        self.no_adv_epochs = args.no_adv_epochs\n",
    "\n",
    "        # Set up logging\n",
    "        self.logger = logger\n",
    "        self._iter = 0\n",
    "        self._ep_idx = 0\n",
    "        self._start_time = time()\n",
    "\n",
    "        # Checkpoint\n",
    "        self.ckpt_exp_dir = f'{args.wandb_project}-{args.wandb_group}-{str(args.random_seed)}'\n",
    "        self.save_period = args.save_period\n",
    "        self.save_ckpt = args.save_ckpt\n",
    "        self.ckpt_dir = args.ckpt_dir\n",
    "        self.ckpt_path = os.path.join(self.ckpt_dir, self.ckpt_exp_dir)\n",
    "        if self.save_ckpt:\n",
    "            if not os.path.exists(self.ckpt_dir):\n",
    "                os.makedirs(self.ckpt_dir)\n",
    "            if not os.path.exists(self.ckpt_path):\n",
    "                os.makedirs(self.ckpt_path)\n",
    "\n",
    "    def common_metrics(self):\n",
    "        \"\"\"Return a dictionary of current metrics.\"\"\"\n",
    "        return dict(\n",
    "            iteration=self._iter,\n",
    "            epoch=self._ep_idx,\n",
    "            total_time=time() - self._start_time,\n",
    "        )\n",
    "\n",
    "    def save_checkpoint(self, name):\n",
    "        if self.save_ckpt:\n",
    "            pth_path = os.path.join(self.ckpt_path, f'multicontextnet-{name}.pth')\n",
    "            torch.save(self.model.state_dict(), pth_path)\n",
    "            pth_path = os.path.join(self.ckpt_path, f'd-{name}.pth')\n",
    "            torch.save(self.model.state_dict(), pth_path)\n",
    "    \n",
    "    def expressive_loss_function(self, output, target): # max squared error over blendshape for each frame, then take the mean\n",
    "        loss = torch.mean(torch.max((output - target) ** 2, dim=-1).values)\n",
    "        return loss\n",
    "\n",
    "    def val(self):\n",
    "        self.model.eval()\n",
    "        val_target_loss_st = []\n",
    "        if self.bs2vertices:\n",
    "            val_V_smooth_loss_st = []\n",
    "        if self.predict_flame:\n",
    "            val_flame_smooth_loss_st = []\n",
    "        val_bs_smooth_loss_st = []\n",
    "        val_bs_expressive_loss_st = []\n",
    "        val_cnt = 0\n",
    "\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            self.val_data, \n",
    "            batch_size=self.batch_size,  \n",
    "            shuffle=True,  \n",
    "            drop_last=True,\n",
    "        )\n",
    "        \n",
    "        for _, data in enumerate(val_loader):\n",
    "            in_audio = data['audio']\n",
    "            if self.normalize_face:\n",
    "                bs_facial = data['facial']\n",
    "            else:\n",
    "                bs_facial = data['facial'] * self.std_facial + self.mean_facial\n",
    "            \n",
    "            in_id = data[\"id\"]\n",
    "            if self.no_text == False:\n",
    "                in_word = data[\"word\"]\n",
    "            in_emo = data[\"emo\"]\n",
    "\n",
    "            in_audio = in_audio.cuda()\n",
    "            bs_facial = bs_facial.cuda()\n",
    "            in_id = in_id.cuda()\n",
    "            if self.no_text == False:\n",
    "                in_word = in_word.cuda()\n",
    "            in_emo = in_emo.cuda()\n",
    "            \n",
    "            if self.predict_flame:\n",
    "                flame_facial = torch.cat((bs_facial @ self.bs_to_flame, bs_facial[:,:,6:14]), dim=-1)\n",
    "                in_pre_face = flame_facial.new_zeros((flame_facial.shape[0], flame_facial.shape[1], flame_facial.shape[2])).cuda()\n",
    "                in_pre_face[:, 0:self.pre_frames] = flame_facial[:, 0:self.pre_frames]\n",
    "\n",
    "                flame_out_face = self.model(in_pre_face,in_audio=in_audio,in_text=in_word, in_id=in_id, in_emo=in_emo)\n",
    "                target_loss = self.target_loss_function(flame_out_face, flame_facial)\n",
    "                flame_smooth_loss = 1 - self.smooth_loss_function(flame_out_face[:,:-1,:], flame_out_face[:,1:,:]).mean()\n",
    "\n",
    "                val_target_loss_st.append(target_loss.item())\n",
    "                val_flame_smooth_loss_st.append(flame_smooth_loss.item()) \n",
    "\n",
    "            else: \n",
    "                in_pre_face = bs_facial.new_zeros((bs_facial.shape[0], bs_facial.shape[1], bs_facial.shape[2] + 1)).cuda()\n",
    "                in_pre_face[:, 0:self.pre_frames, :-1] = bs_facial[:, 0:self.pre_frames]\n",
    "                in_pre_face[:, 0:self.pre_frames, -1] = 1 \n",
    "\n",
    "                if self.no_text:\n",
    "                    bs_pred_face = self.model(in_pre_face,in_audio=in_audio, in_id=in_id, in_emo=in_emo)\n",
    "                else:\n",
    "                    bs_pred_face = self.model(in_pre_face,in_audio=in_audio,in_text=in_word, in_id=in_id, in_emo=in_emo)\n",
    "\n",
    "                if self.bs2vertices:\n",
    "                    V_pred_face = torch.sum(bs_pred_face.unsqueeze(3).unsqueeze(4) * self.V_deltas, axis=2)\n",
    "                    V_gt_face = torch.sum(bs_facial.unsqueeze(3).unsqueeze(4) * self.V_deltas, axis=2)\n",
    "                    \n",
    "                    target_loss = self.target_loss_function(V_pred_face, V_gt_face)\n",
    "                    V_smooth_loss = 1 - self.smooth_loss_function(V_pred_face[:,:-1,:], V_pred_face[:,1:,:]).mean()\n",
    "\n",
    "                    val_target_loss_st.append(target_loss.item())\n",
    "                    val_V_smooth_loss_st.append(V_smooth_loss.item())\n",
    "                else:\n",
    "                    target_loss = self.target_loss_function(bs_pred_face, bs_facial)\n",
    "                    bs_smooth_loss = 1 - self.smooth_loss_function(bs_pred_face[:,:-1,:], bs_pred_face[:,1:,:]).mean()\n",
    "                    bs_expressive_loss = self.expressive_loss_function(bs_pred_face, bs_facial)\n",
    "\n",
    "                    val_target_loss_st.append(target_loss.item())\n",
    "                    val_bs_smooth_loss_st.append(bs_smooth_loss.item())\n",
    "                    val_bs_expressive_loss_st.append(bs_expressive_loss.item())\n",
    "            \n",
    "            val_cnt += 1\n",
    "            if val_cnt >= self.val_size:\n",
    "                break\n",
    "        if self.predict_flame:\n",
    "            return {\n",
    "                \"target_loss\": float(np.average(val_target_loss_st)),\n",
    "                \"flame_smooth_loss\": float(np.average(val_flame_smooth_loss_st)),\n",
    "            }\n",
    "        else:\n",
    "            if self.bs2vertices:\n",
    "                return {\n",
    "                    \"target_loss\": float(np.average(val_target_loss_st)),\n",
    "                    \"V_smooth_loss\": float(np.average(val_V_smooth_loss_st)),\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"target_loss\": float(np.average(val_target_loss_st)),\n",
    "                    \"smooth_loss\": float(np.average(val_bs_smooth_loss_st)),\n",
    "                    \"expressive_loss\": float(np.average(val_bs_expressive_loss_st)),\n",
    "                }\n",
    "\n",
    "    def train(self):\n",
    "        train_metrics = {}\n",
    "        for self._ep_idx in range(self.epochs):\n",
    "            use_adv = bool(self._ep_idx>=self.no_adv_epochs)\n",
    "            for it, data in enumerate(self.train_loader):\n",
    "                self.model.train()\n",
    "                in_audio = data['audio']\n",
    "                if self.normalize_face:\n",
    "                    bs_facial = data['facial']\n",
    "                else:\n",
    "                    bs_facial = data['facial'] * self.std_facial + self.mean_facial\n",
    "                in_id = data[\"id\"]\n",
    "                if self.no_text == False:\n",
    "                    in_word = data[\"word\"]\n",
    "                in_emo = data[\"emo\"]\n",
    "\n",
    "                in_audio = in_audio.cuda()\n",
    "                bs_facial = bs_facial.cuda()\n",
    "                in_id = in_id.cuda()\n",
    "                if self.no_text == False:\n",
    "                    in_word = in_word.cuda()\n",
    "                in_emo = in_emo.cuda()\n",
    "                \n",
    "                if self.predict_flame:\n",
    "                    flame_facial = torch.cat((bs_facial @ self.bs_to_flame, bs_facial[:,:,6:14]), dim=-1)\n",
    "                    in_pre_face = flame_facial.new_zeros((flame_facial.shape[0], flame_facial.shape[1], flame_facial.shape[2])).cuda()\n",
    "                    in_pre_face[:, 0:self.pre_frames] = flame_facial[:, 0:self.pre_frames]\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    flame_out_face = self.model(in_pre_face,in_audio=in_audio,in_text=in_word, in_id=in_id, in_emo=in_emo)\n",
    "                    target_loss = self.target_loss_function(flame_out_face, flame_facial)\n",
    "                    flame_smooth_loss = 1 - self.smooth_loss_function(flame_out_face[:,:-1,:], flame_out_face[:,1:,:]).mean()\n",
    "                    loss = self.target_weight * target_loss  + self.smooth_weight * flame_smooth_loss\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    if it % self.log_period == 0:\n",
    "                        train_metrics = {\n",
    "                            \"target_loss\": float(target_loss.item()),\n",
    "                            \"flame_smooth_loss\": float(flame_smooth_loss.item()),\n",
    "                        }\n",
    "                        train_metrics.update(self.common_metrics())\n",
    "                        self.logger.log(train_metrics, 'train')\n",
    "                        print(f'[{self._ep_idx}][{it}/{len(self.train_loader)}]: [train] [target loss]: {train_metrics[\"target_loss\"]} flame smooth loss]: {train_metrics[\"flame_smooth_loss\"]}')\n",
    "\n",
    "                else: \n",
    "                    in_pre_face = bs_facial.new_zeros((bs_facial.shape[0], bs_facial.shape[1], bs_facial.shape[2] + 1)).cuda()\n",
    "                    in_pre_face[:, 0:self.pre_frames, :-1] = bs_facial[:, 0:self.pre_frames]\n",
    "                    in_pre_face[:, 0:self.pre_frames, -1] = 1 \n",
    "\n",
    "                    # discriminator training\n",
    "                    if use_adv:\n",
    "                        self.d_optimizer.zero_grad()\n",
    "                        if self.no_text:\n",
    "                            bs_pred_face = self.model(in_pre_face,in_audio=in_audio, in_id=in_id, in_emo=in_emo)\n",
    "                        else:\n",
    "                            bs_pred_face = self.model(in_pre_face,in_audio=in_audio,in_text=in_word, in_id=in_id, in_emo=in_emo)\n",
    "                        out_d_fake = self.d_model(bs_pred_face)\n",
    "                        out_d_real = self.d_model(bs_facial)\n",
    "                        d_loss = torch.sum(-torch.mean(torch.log(out_d_real + 1e-8) + torch.log(1 - out_d_fake + 1e-8)))\n",
    "                        d_loss.backward()\n",
    "                        self.d_optimizer.step()\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    if self.no_text:\n",
    "                        bs_pred_face = self.model(in_pre_face,in_audio=in_audio, in_id=in_id, in_emo=in_emo)\n",
    "                    else:\n",
    "                        bs_pred_face = self.model(in_pre_face,in_audio=in_audio,in_text=in_word, in_id=in_id, in_emo=in_emo)\n",
    "\n",
    "                    if self.bs2vertices:\n",
    "                        V_pred_face = torch.sum(bs_pred_face.unsqueeze(3).unsqueeze(4) * self.V_deltas, axis=2)\n",
    "                        V_gt_face = torch.sum(bs_facial.unsqueeze(3).unsqueeze(4) * self.V_deltas, axis=2)\n",
    "\n",
    "                        target_loss = self.target_loss_function(V_pred_face, V_gt_face)\n",
    "                        V_smooth_loss = 1 - self.smooth_loss_function(V_pred_face[:,:-1,:], V_pred_face[:,1:,:]).mean()\n",
    "                        loss = self.target_weight * target_loss  + self.smooth_weight * V_smooth_loss\n",
    "                    \n",
    "                        loss.backward()\n",
    "                        self.optimizer.step()\n",
    "\n",
    "                        if it % self.log_period == 0:\n",
    "                            train_metrics = {\n",
    "                                \"target_loss\": float(target_loss.item()),\n",
    "                                \"V_smooth_loss\": float(V_smooth_loss.item())\n",
    "                            }\n",
    "                            train_metrics.update(self.common_metrics())\n",
    "                            self.logger.log(train_metrics, 'train')\n",
    "                            print(f'[{self._ep_idx}][{it}/{len(self.train_loader)}]: [train] [target loss]: {train_metrics[\"target_loss\"]} [vertices smooth loss]: {train_metrics[\"V_smooth_loss\"]}')\n",
    "                    else:\n",
    "                        # generator training\n",
    "                        adv_loss = None\n",
    "                        target_loss = self.target_loss_function(bs_pred_face, bs_facial)\n",
    "                        bs_expressive_loss = self.expressive_loss_function(bs_pred_face, bs_facial)\n",
    "                        bs_smooth_loss = 1 - self.smooth_loss_function(bs_pred_face[:,:-1,:], bs_pred_face[:,1:,:]).mean()\n",
    "                        loss = self.target_weight * target_loss + self.smooth_weight * bs_smooth_loss + self.expressive_weight * bs_expressive_loss\n",
    "\n",
    "                        if use_adv:\n",
    "                            dis_out = self.d_model(bs_pred_face)\n",
    "                            adv_loss = -torch.mean(torch.log(dis_out + 1e-8)) # self.adv_loss(out_d_fake, real_gt) # here 1 is real\n",
    "                            loss += self.adv_weight * adv_loss\n",
    "                        \n",
    "                        loss.backward()\n",
    "                        self.optimizer.step()\n",
    "\n",
    "                        if it % self.log_period == 0:\n",
    "                            train_metrics.update({\n",
    "                                \"target_loss\": float(target_loss.item()),\n",
    "                                \"smooth_loss\": float(bs_smooth_loss.item()),\n",
    "                                \"expressive_loss\": float(bs_expressive_loss.item()),\n",
    "                            })\n",
    "                            if use_adv:\n",
    "                                train_metrics[\"dis_loss\"] = float(d_loss.item())\n",
    "                                train_metrics[\"adversarial_loss\"] = float(adv_loss.item())\n",
    "                            else:\n",
    "                                train_metrics[\"dis_loss\"] = None\n",
    "                                train_metrics[\"adversarial_loss\"] = None\n",
    "                            train_metrics.update(self.common_metrics())\n",
    "                            self.logger.log(train_metrics, 'train')\n",
    "                            print(f'[{self._ep_idx}][{it}/{len(self.train_loader)}]: [train] [target loss]: {train_metrics[\"target_loss\"]} [adv loss]: {train_metrics[\"adversarial_loss\"]} [smooth loss]: {train_metrics[\"smooth_loss\"]} [exp loss]: {train_metrics[\"expressive_loss\"]} [dis loss]: {train_metrics[\"dis_loss\"]}')\n",
    "                if it % self.val_period == 0:\n",
    "                    val_metrics = self.val()\n",
    "                    val_metrics.update(self.common_metrics())\n",
    "                    self.logger.log(val_metrics,'val')\n",
    "                    if self.predict_flame:\n",
    "                        print(f'[{self._ep_idx}][{it}/{len(self.train_loader)}]: [val] [target loss]: {val_metrics[\"target_loss\"]} [flame smooth loss]: {val_metrics[\"flame_smooth_loss\"]}')\n",
    "                    else:\n",
    "                        if self.bs2vertices:\n",
    "                            print(f'[{self._ep_idx}][{it}/{len(self.train_loader)}]: [val] [target loss]: {val_metrics[\"target_loss\"]} [vertices smooth loss]: {val_metrics[\"V_smooth_loss\"]}')\n",
    "                        else:\n",
    "                            print(f'[{self._ep_idx}][{it}/{len(self.train_loader)}]: [val] [target loss]: {val_metrics[\"target_loss\"]} [bs smooth loss]: {val_metrics[\"smooth_loss\"]} [bs expressive loss]: {val_metrics[\"expressive_loss\"]}')\n",
    "\n",
    "                self._iter += 1\n",
    "            if (self._ep_idx+1) % self.save_period == 0:\n",
    "                self.save_checkpoint(str(self._ep_idx+1))\n",
    "        self.logger.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "\t\"\"\"Set seed for reproducibility.\"\"\"\n",
    "\trandom.seed(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(args.random_seed)\n",
    "logger = Logger(args)\n",
    "train_data = CustomDataset(args, \"train\")\n",
    "val_data = CustomDataset(args, \"val\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert torch.cuda.is_available() == True\n",
    "\n",
    "model = GestureGen(args)\n",
    "d_model = ConvDiscriminator(args)\n",
    "if args.continue_training:\n",
    "        model.load_state_dict(torch.load(args.pretrained_model))\n",
    "\n",
    "trainer = Trainer(args, device, train_data, val_data, model, d_model, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scripts.Dataset import a2bsDataset\n",
    "from scripts.MulticontextNet import GestureGen\n",
    "#import wandb\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = 'ckpt_model/multicontextnet-all-vertex-2024/multicontextnet-300.pth'\n",
    "net = GestureGen(args).cuda()\n",
    "#net.load_state_dict(torch.load(model_path))\n",
    "optimizer = torch.optim.Adam( net.parameters(), lr=1e-4)#, weight_decay=1e-5)\n",
    "train_target_loss = []\n",
    "train_expressive_loss = []\n",
    "train_smooth_loss = []\n",
    "train_mse_loss = []\n",
    "val_target_loss = []\n",
    "val_expressive_loss = []\n",
    "val_smooth_loss = []\n",
    "val_mse_loss = []\n",
    "\n",
    "def plot_train_val_loss():\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(10, 3))\n",
    "    axs[0].plot(train_target_loss, 'r-')\n",
    "    axs[0].set_title('Target Loss')\n",
    "    axs[1].plot(train_expressive_loss, 'p-')\n",
    "    axs[1].set_title('Expressive Loss')\n",
    "    axs[2].plot(train_smooth_loss, 'g-')\n",
    "    axs[2].set_title('Smooth Loss')\n",
    "    axs[3].plot(train_mse_loss, 'b-')\n",
    "    axs[3].set_title('MSE Loss')\n",
    "    fig.suptitle('Training Iterations', fontsize = 16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(10, 3))\n",
    "    axs[0].plot(val_target_loss, 'r-')\n",
    "    axs[0].set_title('Target Loss')\n",
    "    axs[1].plot(val_expressive_loss, 'p-')\n",
    "    axs[1].set_title('Expressive Loss')\n",
    "    axs[2].plot(val_smooth_loss, 'g-')\n",
    "    axs[2].set_title('Smooth Loss')\n",
    "    axs[3].plot(val_mse_loss, 'b-')\n",
    "    axs[3].set_title('MSE Loss')\n",
    "    fig.suptitle('Validation Iterations', fontsize = 16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data))\n",
    "data = next(iter(train_loader))\n",
    "in_audio = data['audio']\n",
    "facial = data['facial']\n",
    "in_id = data[\"id\"]\n",
    "in_word = data[\"word\"]\n",
    "in_emo = data[\"emo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expressive_loss_function(output, target): # max squared error over blendshape for each frame, then take the mean\n",
    "    loss = torch.mean(torch.max((output - target) ** 2, dim=-1).values)\n",
    "    return loss\n",
    "# a = torch.tensor([[[1,2,3],[1,2,3]],[[4,5,6],[4,5,6]]]).float()\n",
    "# b = torch.tensor([[[3,3,4],[2,3,4]],[[5,6,7],[5,6,7]]]).float()\n",
    "# print(a.shape)\n",
    "# max_square_error(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "num_epochs = 700\n",
    "log_period = 200\n",
    "val_period = 600\n",
    "val_size = 25\n",
    "bs_to_flame = torch.from_numpy(np.load('mat_final.npy'))\n",
    "flame_to_bs = bs_to_flame.pinverse()\n",
    "target_loss_function = torch.nn.HuberLoss()\n",
    "smooth_loss_function = torch.nn.CosineSimilarity(dim=2)\n",
    "mse_loss_function = torch.nn.MSELoss()\n",
    "target_weight = 1.5\n",
    "expressive_weight = 0.5\n",
    "smooth_weight = 0.5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for it, data in enumerate(tqdm(train_loader)):\n",
    "        net.train()\n",
    "        in_audio = data['audio']\n",
    "        facial = data['facial'] * std_facial + mean_facial\n",
    "        in_id = data[\"id\"]\n",
    "        in_word = data[\"word\"]\n",
    "        in_emo = data[\"emo\"]\n",
    "\n",
    "        in_audio = in_audio.cuda()\n",
    "        in_facial = torch.cat((facial @ bs_to_flame, facial[:,:,6:14]), dim=-1)\n",
    "        in_facial = in_facial.cuda()\n",
    "        in_id = in_id.cuda()\n",
    "        in_word = in_word.cuda()\n",
    "        in_emo = in_emo.cuda()\n",
    "\n",
    "        pre_frames = 4\n",
    "        if args.predict_flame:\n",
    "            in_pre_face = in_facial.new_zeros((in_facial.shape[0], in_facial.shape[1], in_facial.shape[2])).cuda()\n",
    "            in_pre_face[:, 0:pre_frames] = in_facial[:, 0:pre_frames]\n",
    "        else:\n",
    "            in_pre_face = in_facial.new_zeros((in_facial.shape[0], in_facial.shape[1], in_facial.shape[2] + 1)).cuda()\n",
    "            in_pre_face[:, 0:pre_frames, :-1] = in_facial[:, 0:pre_frames]\n",
    "            in_pre_face[:, 0:pre_frames, -1] = 1 \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out_face = net(in_pre_face,in_audio=in_audio,in_text=in_word, in_id=in_id, in_emo=in_emo)\n",
    "        target_loss = target_loss_function(out_face,in_facial)# + target_loss_function(out_face[:,:,6:14], in_facial[:,:,6:14]) # to account for eye movement\n",
    "        #expressive_loss = expressive_loss_function(out_face, in_facial)\n",
    "        smooth_loss = 1 - smooth_loss_function(out_face[:,:-1,:], out_face[:,1:,:]).mean()\n",
    "        loss = target_weight * target_loss  + smooth_weight * smooth_loss# + expressive_weight * expressive_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_target_loss.append(target_loss.item())\n",
    "        #train_expressive_loss.append(expressive_loss.item())\n",
    "        train_smooth_loss.append(smooth_loss.item())\n",
    "        #train_mse_loss.append(mse_loss_function(out_face.cpu()*std_facial+mean_facial, facial.cpu()*std_facial+mean_facial).item())\n",
    "        \n",
    "        #logging\n",
    "        #if it % log_period == 0:\n",
    "        #    print(f'[{epoch}][{it}/{len(train_loader)}]: [train] [target loss]: {train_target_loss[-1]} [exp loss]: {train_expressive_loss[-1]} [smooth loss]: {train_smooth_loss[-1]} [mse]: {train_mse_loss[-1]}')\n",
    "        \n",
    "        if it % val_period == 0:\n",
    "            net.eval()\n",
    "            val_target_loss_st = []\n",
    "            #val_expressive_loss_st = []\n",
    "            val_smooth_loss_st = []\n",
    "            val_mse_loss_st = []\n",
    "            val_cnt = 0\n",
    "\n",
    "            val_loader = torch.utils.data.DataLoader(\n",
    "                val_data, \n",
    "                batch_size=args.batch_size,  \n",
    "                shuffle=True,  \n",
    "                drop_last=True,\n",
    "            )\n",
    "            \n",
    "            for _, data in enumerate(val_loader):\n",
    "                in_audio = data['audio']\n",
    "                facial = data['facial'] * std_facial + mean_facial\n",
    "                in_id = data[\"id\"]\n",
    "                in_word = data[\"word\"]\n",
    "                in_emo = data[\"emo\"]\n",
    "\n",
    "                in_audio = in_audio.cuda()\n",
    "                in_facial = torch.cat((facial @ bs_to_flame, facial[:,:,6:14]), dim=-1)\n",
    "                in_facial = in_facial.cuda()\n",
    "                in_id = in_id.cuda()\n",
    "                in_word = in_word.cuda()\n",
    "                in_emo = in_emo.cuda()\n",
    "\n",
    "                pre_frames = 4\n",
    "                if args.predict_flame:\n",
    "                    in_pre_face = in_facial.new_zeros((in_facial.shape[0], in_facial.shape[1], in_facial.shape[2])).cuda()\n",
    "                    in_pre_face[:, 0:pre_frames] = in_facial[:, 0:pre_frames]\n",
    "                else:\n",
    "                    in_pre_face = in_facial.new_zeros((in_facial.shape[0], in_facial.shape[1], in_facial.shape[2] + 1)).cuda()\n",
    "                    in_pre_face[:, 0:pre_frames, :-1] = in_facial[:, 0:pre_frames]\n",
    "                    in_pre_face[:, 0:pre_frames, -1] = 1 \n",
    "\n",
    "                out_face = net(in_pre_face,in_audio=in_audio,in_text=in_word, in_id=in_id, in_emo=in_emo)\n",
    "                target_loss = target_loss_function(out_face, in_facial)# + target_loss_function(out_face[:,:,6:14], facial[:,:,6:14])\n",
    "                #expressive_loss = expressive_loss_function(out_face, facial)\n",
    "                smooth_loss = 1 - smooth_loss_function(out_face[:,:-1,:], out_face[:,1:,:]).mean()\n",
    "\n",
    "                val_target_loss_st.append(target_loss.item())\n",
    "                #val_expressive_loss_st.append(expressive_loss.item())\n",
    "                val_smooth_loss_st.append(smooth_loss.item())\n",
    "                #val_mse_loss_st.append(mse_loss_function(out_face.cpu()*std_facial+mean_facial, facial.cpu()*std_facial+mean_facial).item())\n",
    "                \n",
    "                \n",
    "                val_cnt += 1\n",
    "                if val_cnt >= val_size:\n",
    "                    break\n",
    "            \n",
    "            val_target_loss.append(np.average(val_target_loss_st))\n",
    "            #val_expressive_loss.append(np.average(val_expressive_loss_st))\n",
    "            val_smooth_loss.append(np.average(val_smooth_loss_st))\n",
    "            #val_mse_loss.append(np.average(val_mse_loss_st))\n",
    "            #print(f'[{epoch}][{it}/{len(train_loader)}]: [val] [target loss]: {val_target_loss[-1]} [exp loss]: {val_expressive_loss[-1]} [smooth loss]: {val_smooth_loss[-1]} [mse]: {val_mse_loss[-1]}')\n",
    "    plot_train_val_loss()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'ckpt_model/multicontextnet-flame-7.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonosc import udp_client\n",
    "import time\n",
    "import sounddevice as sd\n",
    "import torch\n",
    "from dataloaders.beat import CustomDataset\n",
    "from dataloaders.build_vocab import Vocab\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "config_file = open(\"gesturegen_config.obj\", 'rb') \n",
    "args = pickle.load(config_file)\n",
    "args.batch_size = 16\n",
    "\n",
    "mean_facial = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.facial_rep}/json_mean.npy\")).float()\n",
    "std_facial = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.facial_rep}/json_std.npy\")).float()\n",
    "mean_audio = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.audio_rep}/npy_mean.npy\")).float()\n",
    "std_audio = torch.from_numpy(np.load(args.root_path+args.mean_pose_path+f\"{args.audio_rep}/npy_std.npy\")).float()\n",
    "bs_to_flame = torch.from_numpy(np.load('mat_final.npy')).float()\n",
    "flame_to_bs = bs_to_flame.pinverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomDataset(args, \"test\")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=1,  \n",
    "    shuffle=True,  \n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = data['audio']\n",
    "facial = data['facial']\n",
    "id = data[\"id\"]\n",
    "word = data[\"word\"]\n",
    "emo = data[\"emo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_facial = facial * std_facial + mean_facial\n",
    "out_audio = audio * std_audio + mean_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try playing the audio, which is at 16KHZ\n",
    "print(out_facial.min(), out_facial.max())\n",
    "print(out_facial.std(), out_facial.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend =  [\n",
    "        \"browDownLeft\",\n",
    "        \"browDownRight\",\n",
    "        \"browInnerUp\",\n",
    "        \"browOuterUpLeft\",\n",
    "        \"browOuterUpRight\",\n",
    "        \"cheekPuff\",\n",
    "        \"cheekSquintLeft\",\n",
    "        \"cheekSquintRight\",\n",
    "        \"eyeBlinkLeft\",\n",
    "        \"eyeBlinkRight\",\n",
    "        \"eyeLookDownLeft\",\n",
    "        \"eyeLookDownRight\",\n",
    "        \"eyeLookInLeft\",\n",
    "        \"eyeLookInRight\",\n",
    "        \"eyeLookOutLeft\",\n",
    "        \"eyeLookOutRight\",\n",
    "        \"eyeLookUpLeft\",\n",
    "        \"eyeLookUpRight\",\n",
    "        \"eyeSquintLeft\",\n",
    "        \"eyeSquintRight\",\n",
    "        \"eyeWideLeft\",\n",
    "        \"eyeWideRight\",\n",
    "        \"jawForward\",\n",
    "        \"jawLeft\",\n",
    "        \"jawOpen\",\n",
    "        \"jawRight\",\n",
    "        \"mouthClose\",\n",
    "        \"mouthDimpleLeft\",\n",
    "        \"mouthDimpleRight\",\n",
    "        \"mouthFrownLeft\",\n",
    "        \"mouthFrownRight\",\n",
    "        \"mouthFunnel\",\n",
    "        \"mouthLeft\",\n",
    "        \"mouthLowerDownLeft\",\n",
    "        \"mouthLowerDownRight\",\n",
    "        \"mouthPressLeft\",\n",
    "        \"mouthPressRight\",\n",
    "        \"mouthPucker\",\n",
    "        \"mouthRight\",\n",
    "        \"mouthRollLower\",\n",
    "        \"mouthRollUpper\",\n",
    "        \"mouthShrugLower\",\n",
    "        \"mouthShrugUpper\",\n",
    "        \"mouthSmileLeft\",\n",
    "        \"mouthSmileRight\",\n",
    "        \"mouthStretchLeft\",\n",
    "        \"mouthStretchRight\",\n",
    "        \"mouthUpperUpLeft\",\n",
    "        \"mouthUpperUpRight\",\n",
    "        \"noseSneerLeft\",\n",
    "        \"noseSneerRight\",\n",
    "        \"tongueOut\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_audio(out_audio, init_time):\n",
    "    time.sleep(init_time - time.time())\n",
    "    sd.play(out_audio, 16000)\n",
    "    sd.wait()\n",
    "    print(\"Audio finished:\", time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def send_udp(out_face, init_time):\n",
    "    #outWeight = np.zeros(52)\n",
    "\n",
    "    ##need to implement get value in\n",
    "    outWeight = out_face\n",
    "\n",
    "    outWeight = outWeight * (outWeight >= 0)\n",
    "\n",
    "    client = udp_client.SimpleUDPClient('127.0.0.1', 5008)\n",
    "    osc_array = outWeight.tolist()\n",
    "    \n",
    "    fps = 15\n",
    "    time.sleep(init_time - time.time())\n",
    "    #start_time = time.time()\n",
    "    for i in range(len(osc_array)):\n",
    "        #print(out_face[i].shape)\n",
    "        for j, out in enumerate(osc_array[i]):\n",
    "            client.send_message('/' + str(blend[j]), out)\n",
    "\n",
    "        elpased_time = time.time() - init_time\n",
    "        sleep_time = 1.0/fps * (i+1) - elpased_time\n",
    "        if sleep_time > 0:\n",
    "            time.sleep(sleep_time)\n",
    "        #start_time = time.time()\n",
    "    print(\"Facial finished:\", time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "init_time = time.time() + 1\n",
    "\n",
    "limit_sec = 20\n",
    "\n",
    "udp_thread = threading.Thread(target=send_udp, args=(out_facial[0, 0:limit_sec*15],init_time))\n",
    "udp_thread.daemon = True  # Set the thread as a daemon to allow it to exit when the main program exits\n",
    "\n",
    "audio_thread = threading.Thread(target=play_audio, args=(out_audio[0, 0:limit_sec*16000],init_time-0.3))\n",
    "audio_thread.daemon = True\n",
    "\n",
    "udp_thread.start()\n",
    "audio_thread.start()\n",
    "\n",
    "udp_thread.join()\n",
    "audio_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(out_audio[0])/16000, len(out_facial[0])/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # load in model\n",
    "from scripts.MulticontextNet import GestureGen\n",
    "#model_path = 'ckpt_model/multicontextnet-flame-7.pth'\n",
    "model_path = 'tmp/multicontextnet-no-text.pth'\n",
    "#model_path = 'ckpt_model/multicontextnet-bs2vertices-2024/multicontextnet-500.pth'\n",
    "#model_path = 'ckpt_model/multicontextnet100.pth'\n",
    "#args.predict_flame = False\n",
    "args.normalize_face = False\n",
    "net = GestureGen(args)\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "net = net.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_audio = audio.cuda()\n",
    "bs_facial = facial * std_facial.float() + mean_facial.float()\n",
    "#flame_facial = torch.cat((bs_facial @ bs_to_flame, bs_facial[:,:,6:14]), dim=-1)\n",
    "in_id = id.cuda()\n",
    "in_word = word.cuda()\n",
    "in_emo = emo.cuda()\n",
    "#in_emo = torch.zeros_like(emo) + 1\n",
    "#in_emo = in_emo.cuda()\n",
    "pre_frames = 4\n",
    "in_pre_facial = bs_facial.new_zeros((bs_facial.shape[0], bs_facial.shape[1], bs_facial.shape[2] + 1)).cuda() \n",
    "in_pre_facial[:, 0:pre_frames, :-1] = bs_facial[:, 0:pre_frames]\n",
    "in_pre_facial[:, 0:pre_frames, -1] = 1 \n",
    "#in_pre_facial = in_facial.new_zeros((flame_facial.shape[0], flame_facial.shape[1], flame_facial.shape[2])).cuda() \n",
    "#in_pre_facial[:, 0:pre_frames] = flame_facial[:, 0:pre_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_facial = net(in_pre_facial, in_audio=in_audio, in_text=in_word, in_id=in_id, in_emo=in_emo).cpu().detach()\n",
    "#pred_facial = pred_facial * std_facial + mean_facial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bs_facial.cpu().min(), bs_facial.cpu().max())\n",
    "print(bs_facial.cpu().std(), bs_facial.cpu().mean())\n",
    "print(pred_facial.min(), pred_facial.max())\n",
    "print(pred_facial.std(), pred_facial.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_bs_facial = pred_facial[:,:,:103] @ flame_to_bs\n",
    "# pred_bs_facial[:,:,6:14] = pred_facial[:,:,103:111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "init_time = time.time() + 1\n",
    "\n",
    "limit_sec = 70\n",
    "\n",
    "udp_thread = threading.Thread(target=send_udp, args=(pred_facial[0,0:limit_sec*15],init_time))\n",
    "udp_thread.daemon = True  # Set the thread as a daemon to allow it to exit when the main program exits\n",
    "\n",
    "audio_thread = threading.Thread(target=play_audio, args=(out_audio[0,0:limit_sec*16000],init_time-0.3))\n",
    "audio_thread.daemon = True\n",
    "\n",
    "udp_thread.start()\n",
    "audio_thread.start()\n",
    "\n",
    "udp_thread.join()\n",
    "audio_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expressive_loss_function(pred_bs_facial, bs_facial))\n",
    "print(torch.nn.functional.mse_loss(pred_bs_facial, bs_facial))\n",
    "print(torch.nn.functional.mse_loss(pred_flame_facial, flame_facial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expressive_loss_function(pred_bs_facial, bs_facial))\n",
    "print(torch.nn.functional.mse_loss(pred_bs_facial, bs_facial))\n",
    "print(torch.nn.functional.mse_loss(pred_flame_facial, flame_facial))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
